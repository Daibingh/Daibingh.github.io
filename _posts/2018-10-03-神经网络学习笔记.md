---
layout: post
title:  "神经网络学习笔记"
categories: 机器学习
tags: 神经网路
author: hdb
mathjax: true
excerpt: 本笔记主要是我研读《神经网络与深度学习》一书之后，对重要知识点的整理和公式的推导。这里讲的神经网络是最简单的前馈神经网络，学习算法采用基于误差反向传播的（随机）梯度下降算法。
comments: true
---

* content
{:toc}

## 前言

本笔记主要是我研读《神经网络与深度学习》一书之后，对重要知识点的整理和公式的推导。这里讲的神经网络是最简单的前馈神经网络，学习算法采用基于误差反向传播的（随机）梯度下降算法。

## 1 神经网络结构和符号定义

一个三层的神经网络结构（包含输入层）如下：

<center><img src="/images/神经网络典型结构.png" width="400px"></center>

注意：输入层节点没有运算功能，直接将输入信号传递给隐藏层，而隐藏层和输出层将输入首先进行线性变换，然后再经过激活函数映射到输出。

神经网络中的符号定义

$L$ : 神经网络层数（包含输入层）

$x=(x_1,x_2,...,x_m)^T$ : 输入

$\widehat{y}=(\widehat{y}_1,\widehat{y}_2,...,\widehat{y}_n)^T$ : 输出

$a^l=(a^l_1,a^l_2,...)^T$ : 第 $l$ 层输出，特别地，$a^1=x, a^L = \widehat{y}$

$z^l=(z^l_1,z^l_2,...)^T$ : 第 $l$ 层带权输入

$W^l$ :  第 $l$ 层与第 $l-1$ 层之间的权重矩阵，$w^l_{ij}$ : 第 $l$ 层第 $i$ 个节点与第 $l-1$ 层第 $j$ 个节点之间的权重.

$b^l$ : 第 $l$ 层偏置向量，$b^l_j$ :  第 $l$ 层第 $j$ 个节点偏置

## 2 信号前向传播与误差反向传播公式

假设损失函数采用二次代价函数（均方差），激活函数采用 $sigmoid$ 函数。

二次代价函数：
$$
C_x=\frac{1}{2}\left\|a^L-y\right\|^2
$$

$sigmoid$ 函数定义：
$$
\sigma{(x)}=\frac{1}{1+e^{-x}}
$$

$sigmoid$ 函数导数有：
$$
\sigma^{'}{(x)}=\sigma{(x)}(1-\sigma{(x))}
$$

信号前向传播公式：
$$
\begin{cases}
a^1=x \\
z^l=W^la^{l-1}+b^l , l\ge{2}\\
a^l=\sigma{(z^l)} ,  l\ge{2}\\
\widehat{y}=a^L
\end{cases}
$$

误差的反向传播公式：
$$
\begin{cases}
\delta^L=\frac{\partial C}{\partial a^L}\odot \sigma^{'}(z^L) =(a^L-y) \odot \sigma^{'}(z^L) \\
\delta^l=[(W^{l+1})^T\delta^{l+1}] \odot \sigma{'}(z^l), l<L \\
\frac{\partial C}{\partial b^l}=\delta^l\\
\frac{\partial C}{\partial W^l}=\delta ^l(a^{l-1})^T
\end{cases}
$$

误差反向传播公式中引入中间变量 $\delta^l$ , 定义为 $\frac{\partial C}{\partial z^l}$ .

公式推导的基本思想是“链式求导法则”，证明时直接进行矩阵求导不易，可先证明分量形式，最后在写成矩阵或向量形式。

## 3 梯度下降算法

上面的误差反向传播公式是为梯度下降算法而服务的，梯度下降算法是神经网络最常用的学习算法。具体来讲又分为：

- 批量梯度下降算法
- 小批量梯度下降算法
- 随机梯度下降算法
- 小批量随机梯度下降算法等


参数更新公式：

$$
\begin{cases}
W^l \leftarrow W^l - \eta \frac{\partial C}{\partial W^l} \\
b^l \leftarrow b^l - \eta \frac{\partial C}{\partial b^l}
\end{cases}
$$

其中，$\eta$ 为学习速率.

$$
\begin{cases}
W^l \leftarrow W^l -\eta\frac{\partial C}{\partial W^l} \\
b^l \leftarrow b^l - \eta\frac{\partial C}{\partial b^l}
\end{cases}
$$

## 4 采用小批量随机梯度下降算法的神经网络训练流程

神经网络学习过程的流程图：

<center><img src="{{site.baseurl}}/images/神经网络学习算法流程图.png"></center>

## 5 经典神经网络存在的问题和改进

### 5.1 神经元饱和问题

经典的神经网络采用的激活函数是 $sigmoid$ 函数，代价采用二次代价函数，两者配合使用共同导致**在输出误差较大时学习的速度反而很慢，随着误差的逐渐减小，学习速度出现先增大后又减小的现象**。（如下图） 为什么会出现这种反常识的现象呢？按照人类的学习经验，不应该是误差越大学习速度越大吗?

<center><img src="{{site.baseurl}}/images/饱和问题.png" width="400px"></center>

要解释这个问题我们首先看看 $sigmoid$ 函数的输入输出曲线：

<center><img src="{{site.baseurl}}/images/sigmoid.png" width="500px"></center>

$sigmoid$ 函数将输入$(-\infty, +\infty)$的数值挤压到$(0, 1)$ 之间。当输入的绝对值很大时，$sigmoid$ 函数的导数趋近于0，再来看看上面的误差反向传播公式，$\delta^L$ 的公式中恰好含有$\sigma^{'} (z^L)$, 这就是原因所在。

改进措施之一：采用**交叉熵代价函数**，效果是将$\delta^L$ 的公式中的$\sigma^{'} (z^L)$ 项约掉。其对应的误差反向传播公式为：

$$
\begin{cases}
\delta^L=\frac{\partial C}{\partial a^L}\odot \sigma^{'}(z^L) = a^L-y  \\
\delta^l=[(W^{l+1})^T\delta^{l+1}] \odot \sigma{'}(z^l), l<L \\
\frac{\partial C}{\partial b^l}=\delta^l\\
\frac{\partial C}{\partial W^l}=\delta ^l(a^{l-1})^T
\end{cases}
$$

可见将代价函数变为交叉熵之后，对比两组公式，只有$\delta^L$ 发生了改变。

推导 $\delta ^L$ 的过程（先证明分量形式）：

<center><img src="/images/交叉熵推导.jpg"> </center>

改进措施之二：输出层采用**$softmax$激活函数**和**对数代价函数**。

softmax 定义如下：

$$
softmax(x_j)=\frac{e^{x_j}}{\sum_{k}{e^{x_k}}}
$$

特点：对每层神经元的输出值进行归一化（之和为1），因此，最终的输出值可以看作是“概率”。

与sigmoid函数类似，其导数也有类似性质：

$$
\frac{\partial softmax(x_i)}{\partial x_j} = 
\begin{cases}
softmax(x_i)(1-softmax(x_i)), i=j \\
-softmax(x_i)softmax(x_j), i\ne j
\end{cases}
$$

对数代价函数的定义：
$$
C_x = -ln (a^L_y)
$$

**巧妙的是输出层采用softmax激活函数和对数代价函数与sigmoid激活函数和交叉熵代价函数的反向传播公式是一样的。**

下面推导采用softmax激活函数和对数代价函数的 $\delta^L$ 的计算式：
$$
\delta^{L}=a^L-y
$$

推导过程：

<center><img src="/images/softmax—ln.jpg"></center>

有了这样的相似性，你应该使一个具有交叉熵代价的 sigmoid 型输出层，还是一个具有对数似然
代价的柔性最大值输出层呢？柔性最大值加上对数似然的组合更加适合于那些需要将输出激活值解释为概率的场景。

### 5.2 过度拟合问题

过度拟合（overfit）是指神经网络在训练过程中过分追求较高的分类准确度，学习到“噪声”等非本质特征的信号，而丧失泛化能力，在训练样本之外表现的很差。一般出现在训练样本很少的情况下。

解决过度拟合有以下几种策略：

- 规范化
- 弃权
- 认为增加训练样本等

规范化中的L2规范化是最常用的手段。基本思想是在原来的代价函数的基础上引入网络**所有权重的平方和项**。即，
$$
C=C_0+\frac{\lambda}{2n}\sum_{w}{w^2}
$$

其中，$C_0$是原来的代价函数，$\lambda>0$ 是规范化参数。
则，权重的更新公式变成

$$
w  \leftarrow w - \eta( \frac{\partial C_0}{\partial w}+\frac{\lambda}{n}w) \\
=(1-\frac{\eta \lambda}{n})w-\eta  \frac{\partial C_0}{\partial w}
$$

这种调整有时被称为**权重衰减**，因为它使得权重变小。

## 6 补充：熵与交叉熵的理解

熵、交叉熵属于信息论中的概念。首先明确几个概念：

信息量：与事件空间中的某一事件相对应。刻画某一事件发生的不确定行。定义为 $I(x)=-log(p(x))$ , 事件发生的概率越小，信息量越大。

熵：与某一随机变量相对应。刻画某一随机变量的不确定性。定义为 $H_X=E[I]=-\sum_{k}{p(x_k)log(p(x_k))}$

当某一随机变量服从均匀分布时，该随机变量的熵最大。

交叉熵：刻画两个随机变量分布的相似性。定义为 $CEH(p,q)=-\sum_k{p(x_k)log(q(x_k))}$ , 其中，p, q分别是两个分布函数。p是真实样本分布，q是待估计样本分布。交叉熵越小，反映两个分布越接近。 
