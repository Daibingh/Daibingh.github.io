---
layout: post
title:  "网络压缩"
categories: DL
tags: DL 网络压缩
author: hdb
comments: true
excerpt: "模型压缩常见算法。"
mathjax: true
---

* content
{:toc}


## 网络剪枝

(network pruning)

一个网络往往会过参数化(over-parameterized)，通过网络剪枝可以移除冗余的**权重**或**神经元**

为什么需要网络剪枝？为何不直接训练一个简单的网络？

一种观点是**小网络不容易进行训练，一个大网络可以看作许多小网络的组合，增加了能训练好的几率**，但是，也存在相反的观点。

网络剪枝的流程

<img src="/images/network-pruning.png" width="500px">

关键在于如何衡量权重和神经元的重要程度

权重剪枝不容易实现和 `GPU`加速，神经元剪枝容易实现

## 知识蒸馏

(knowledge distillation)

做法是：有一个大网络和一个小网络，大网络已经训练好，小网络要向大网络学习，使得对于同一个输入，它们的输出尽可能一致

<img src="/images/knowledge-distillation.png" width="500px">

这样做的理由是，不仅告诉小网络样本的真实类别，还告诉它该样本长得像谁

<img src="/images/knowledge-distillation-2.png" width="500px">

## 参数量化

(Parameter Quantization)

<img src="/images/parameter-quant.png" width="500px">

## 架构设计

(architecture design)

### 使用瓶颈

不论是全连接还是卷积网络，都可以采用一种瓶颈结构，即在两个层之间加一个收缩层（神经元数目或通道数减少）

<img src="/images/low-rank-app.png" width="500px">

### 深度可分离卷积

(Depth-wise Separable Convolution)

<img src="/images/dwconv.png" width="500px">

<img src="/images/dwconv-2.png" width="500px">

<img src="/images/dwconv-3.png" width="500px">

To learn more ...

- SqueezeNet https://arxiv.org/abs/1602.07360
- MobileNet https://arxiv.org/abs/1704.04861
- ShuffleNet https://arxiv.org/abs/1707.01083
- Xception https://arxiv.org/abs/1610.02357

## 动态计算

(Dynamic Computation)
