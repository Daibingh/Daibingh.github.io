---
layout: post
title:  "浅谈半监督学习"
categories: DL
tags: DL 半监督学习
author: hdb
comments: true
excerpt: "介绍一下半监督学习是什么，怎么用，什么时候用。"
mathjax: true
---

* content
{:toc}



## 引言

半监督学习（semi-supervised learning）是指手头上有一部分带标签的数据，还有很大一部分没有标签的数据，现在要利用这些数据，尽可能学习到有用的信息。

根据 unlabeled 数据是否是测试集，分为两类：

- Transductive learning: unlabeled 数据是测试集
- Inductive learning: unlabeled 数据不是测试集

## 生成模型中的半监督学习

（Semi-Supervised Learning for Generative Model）

<img src="/images/semi-sup-gene-1.png" width="500px">

<img src="/images/semi-sup-gene-2.png" width="500px">

补充：关于似然（likelihood）的理解

一个随机过程有状态空间和参数空间，$P(x; \theta)$ 即可以称为概率，也可以称为似然。当称为概率时，我们关注的是状态空间，当称为似然时，关注的是参数空间。

**概率**(密度)表达给定![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)下样本随机向量![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7BX%7D+%3D+%5Ctextbf%7Bx%7D)的**可能性**，而**似然**表达了给定样本![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7BX%7D+%3D+%5Ctextbf%7Bx%7D)下参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_1)(相对于另外的参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_2))为真实值的**可能性**。我们总是对随机变量的取值谈**概率**，而在非贝叶斯统计的角度下，参数是一个实数而非随机变量，所以我们一般不谈一个参数的**概率**。

似然 = 样本的联合概率密度，即
$$
L(\theta | \mathbf{x})=f(\mathbf{x} ; \theta)
$$

## 边界低密度假设

（Low-density Separation）

“非黑即白”

边界低密度假设：两个类别交界处样本分布稀疏

利用该假设有下面两种应用

### Self-training

应用 **self-training** 到分类问题

<img src="/images/self-training.png" width="500px">

### 基于熵的正则化

（Entropy-based Regularization）

loss 添加正则项，使得 unlabeled 数据预测的类别分布不均匀。

<img src="/images/entropy-based-regular.png" width="500px">

##  平滑度假设

（Smoothness Assumption）

“近朱者赤，近墨者黑”

相似的 $x$ 具有相同的 $\hat{y}$

<img src="/images/smooth-assum.png" width="500px">

该假设的应用如下

### 聚类

<img src="/images/cluster-labeling.png" width="500px">

值得指出的是，对于图像不容易直接聚类，可以先采用 deep auto-encoder 

### 基于图的方法

<img src="/images/graph-based.png" width="500px">

<img src="/images/graph-based-2.png" width="500px">

## Better Representation

迁移学习中的属性表示